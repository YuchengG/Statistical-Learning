#+TITLE: Exercise on Convex Optimization
#+author: Songpeng Zu
#+date: Start at 2016-01-15
#+startup: latexpreview
#+LATEX_CLASS: article
#+LATEX_CLASS_OPTIONS: [a4paper]
#+LATEX_HEADER: \usepackage{color}

#+begin_abstract
this  material is personal answers for the exercise on convex optimization wirtten by stephen boyd and lieven vandenberghe.
i only put the main skeletons here.
#+end_abstract

* Convex sets
** Definition of convexity
1.  $\sum_{i=1}^{k}\theta_{k}x_{k} = (\sum_{i=1}^{k-1}\theta_{i})\sum_{i=1}^{k-1}\frac{\theta_{i}}{\sum_{i=1}^{k-1}\theta_{i}}x_{i} + \theta_{k}x_{k}$
     in which, $\sum_{i=1}^{k-1}\frac{\theta_{i}}{\sum_{i=1}^{k-1}\theta_{i}}x_{i} \in \mathcal{c}$  acording to the mathmatical induction.

2.  Both are direct, since \theta x + (1-\theta)y \in a line.

3.  We can construct a sequence by the binary search [fn:1], then for any \theta > 0, \theta x + (1 - \theta) y is either in the sequence or the limitation of the sequence. Note  a closed set in the Euclean Space is tight, which means
       the limitation of the sequence (all the points in the sequences belong to the closed set) is in the set.

4.  The affine hull is the minumum affine set that contains $\mathcal{C}$, and the same as the intersection of all the affine sets that contain $\mathcal{C}$.

** Examples
5. |b_{1} - b_{2}|

6.  When $a = c \tilde{a}, c \ne 0, d \leq \tilde{d}$, the first halfspace belongs to the second one.  When $(a,b) = c(\tilde{a}, \tilde{b}), c \ne 0$, they are equal.

7. $||x-a||_{2} \leq ||x-b||_{2}$ equals to  $x^{T}x - 2a^{T}x + a^{T}a \leq x^{T}x - 2b^{T}x + b^{T}b$. Then we can get  ${(b - a)}^{T}x \leq \frac{b^{T}b - a^{T}a}{2}$

8. (a) When both are zeros, yes; when one is zero, then it's a line segment, yes; when both are non-zeros,  it forms a parallelogram (specially, $a_{1} \bot a_{2}$, it's a square), so yes.\\
    (b) Yes.
         \begin{equation}
        S = \{ x | -\mathcal{I} x \preceq 0,
         \begin{bmatrix}
         1^{T}\\
         a^{T}\\
         {a^2}^{T}
         \end{bmatrix}  x =
        \begin{bmatrix}
        1\\
        b_{1}\\
        b_{2}
        \end{bmatrix} \}
       \end{equation}
    (c)  Yes. $S = \{x | x \preceq 1, -x \preceq 0 \}$. Note, y is on the unit ball. $x^{T}y$ is the line passing through origin with y as its normal. In fact, it involves the space. \\
    (d) Yes, the same as above.

9. (a) From Exercise 7, we know that V is the intersection of all the halfspaces,  then it's polyhedron.\\
    (b) We need to choose a interior point from S (S has nonempty interior ). Then for each halfspace expression, we can reversely construct the corresponding equations x_{i} according to Exercise 7.\\
    (c) Since no specific rules for choosing x_{0}. We can then say V_{k} is polyhedon, for k \in 0, ..., K.  For each pair of V_{i} and V_{k} (i is not equal to k), their intersection is the boundary between them.
         For any point in $\mathcal{R}$ it must be near to one point from x_{0},..., x_{K}. Then the union of V_{k} is the whole space.
        $\color{red}{The~latter~question.}$

10. (a) If $A \succeq 0$, i.e., A is a symmetric positive semidefinite matrix. For any x_{1}, x_{2}, we have,
           \begin{equation*}
           {(x_{1} - x_{2})}^T A (x_{1} - x_{2}) = x_{1}^T A x_{1} + x_{2}^T A x_{2} - 2 x_{1}^T A x_{2}\geq 0
           \end{equation*}
          Then, we get
          \begin{equation}
          2 x_{1}^T A x_{2} \leq x_{1}^T A x_{1} + x_{2}^T A x_{2}
          \end{equation}
          Then for any convex combination [fn:2] of x_{1} and x_{2}, y,  we have:
          \begin{equation*}
          \begin{aligned}
         &  y^Ty + b^Ty + c \\
          & = \theta^2 x_{1}^TAx_{1} + {(1-\theta)^2} x_{2}^TAx_{2} + 2 \theta (1- \theta)x_{1}^TAx_{2} + b^T(\theta x_{1} + (1-\theta)x_{2}) + (\theta + 1 - \theta)c\\
          & \leq \theta^2 x_{1}^TAx_{1} + {(1-\theta)^2} x_{2}^TAx_{2} + \theta (1- \theta)(x_{1}^T A x_{1} + x_{2}^T A x_{2}) +  b^T(\theta x_{1} + (1-\theta)x_{2}) + (\theta + 1 - \theta)c\\
          & \leq \theta (x_{1}^TAx_{1} + b^Tx_{1} + c) + (1 - \theta)(x_{2}^TAx_{2} + b^{T}x_{2} + c)\\
          & \leq 0
          \end{aligned}
          \end{equation*}
          So C is a convex set when $A \succeq 0$. \\
      (b) For any convex combination of x_{1} and x_{2}, the linear equation is trivial. For the quadratic inequality, we follow the same logic as (a). The different is, instead of use $x_{1}^TAx_{2}$
           directly, here we use $x_{1}^T(A+ \lambda gg^{T})x_{2} - \lambda x_{1}^Tgg^{T}x_{2}$. The  matrix $A+ \lambda gg^{T}$  is positive semidefinite, and we can again use its character as (a).
           Also note:
          \begin{equation*}
          x^Tgg^Tx = trace(g^Tx x^Tg) = trace(g^Tx (g^Tx)) = g^Tx (g^Tx)
          \end{equation*}
          Given the linear equation, we then have
          \begin{equation*}
          x_{1}^Tgg^Tx_{2} = g^Tx_{1} (g^T x_{2}) = h^2
          \end{equation*}
          Then we can directly prove the quadratic inequality. $\color{red}{The~latter~question.}$

11. Change the original inequality as $\log x_{1} + \log x_{2} \geq 0$, then use the general arithmetic-geometric mean inequality. For $n\geq 3$, we can also generalize the inequality based on the provement of
    the two-dimensional situation, which uses the $-\log$ function's convexisity.

12. (a) yes.\\
    (b) yes.\\
    (c) yes.\\
    (d) yes. Suppose x_1, x_2 belongs to the defined set. Then from ${||x - x_0||}_2 \leq ||x - y||_2$, we have that
    \begin{gather}
    -2x_1^T(x_0 - y) \leq y^Ty - x_0^Tx_0 \\
    -2x_2^T(x_0 - y) \leq y^Ty - x_0^Tx_0
    \end{gather}
    Then if x is the convex combination of x_1 and x_2, we have
    \begin{equation}
    \begin{aligned}
    {||x - x_0||}_2^2 & = {||\theta x_1 + (1-\theta)x_2 - x_0||}_2^2\\
    & = \theta^2 x_1^Tx_1 + {(1-\theta)}^2 x_2^T x_2 + 2\theta(1-\theta)x_1^Tx_2 + x_0^Tx_0 - 2(\theta x_1^T + (1-\theta)x_2^T)x_0
    \end{aligned}
    \end{equation}
    Replace x_0 with y, we can have similar formula. Compare the two formulas, only last two terms are different. Considering the previous inequalities,
    we can see x still satisfies the condition.\\
    (e) yes. The procedure is similar with the one above. \\
    Note we only need to prove x, the convex combination of x_1 and x_2, satisfies:
    \begin{equation}
    \inf_{x_0}(x_0^Tx_0 - 2x^Tx_0) \leq y^Ty - 2x^Ty
    \end{equation}
    for any y in the T. And $x_0^Tx_0 = \theta x_0^Tx_0 + (1-\theta) x_0^Tx_0$, x_1 and x_2 satisfies the inequality above.\\
    (f) yes. For any y in S_2, x is the convex combination of x_1 and x_2, and x_1 and x_2 are in the defined set, we have\\
    $x + y = \theta(x_1 + y) + (1-\theta)(x_2 + y), x_1 +y \in S_1, x_2 + y \in S_1$ Note S_1 is convex, so $x+y \in S_1$.\\
    (g) yes. Follow the same procedure as (d).

13. $\{\theta_1 X_1X_1^T + \theta_2 X_2X_2^T | \theta_i \geq 0, X_i \in \mathcal{R}^{n\times k}, rank(X_i)=k, i=1,2\}$

14. (a) Follow the logic at *12(f)*\\
    (b) Follow the logic above. Note S is convex, then the convex combination of points in S still in S.\\

15. (a) Yes. Expectation is a linear function for a discrete distribution p.\\
    (b) Yes. $\Pr(x\geq \alpha) = \sum_{i=1}^np_i\mathcal{I}_{a_i \geq \alpha}$, so it is a linear function for p.\\
    (c) Yes, linear inequality for p.\\
    (d) Yes, the same as above.\\
    (e) Yes, the same as above. \\
    (f) Yes, $Var(X) = EX^2 - (EX)^2$, linear inequality for p.\\
    (g) Yes, the same as above.\\
    (h) $\color{red}{Not sure.}$ \\
    (i) Yes. for any $\beta$ so that $\Pr(x\leq \beta)\geq 0.25$, we have that
        \begin{equation}
        \begin{aligned}
        Pr(x\leq \beta) &  = \sum_{i=1}^n p_i\mathcal{I}_{a_i\leq \beta}\\
        & = \sum_{i=1}^n \theta p^{(1)}_i\mathcal{I}_{a_i\leq \beta} + \sum_{i=1}^n (1-\theta)p^{(2)}_i\mathcal{I}_{a_i\leq \beta}\\
        & \geq 0.25
        \end{aligned}
        \end{equation}
        So the $\inf$ of $\beta$ on the convex combination of p^{(1)} and p^{(2)} must less than $\alpha$.

** Operations that preserve convexity
* Convex functions
* Convex optimization problems
* Duality
* Approximation and fitting
* Statistical estimation
* Genomic problems

[fn:1] The binary search means that for two given points  x and y, if we want to find a point between them, we can firstly use their middle point. If the middle point is not, then the point must lie in either
          [x, middle point] or [middle point, y] (we assume that x < y). Then we can follow the same procedure with x or y replaced with the middle point. Then the point will be in found by our procedure, or
         be the limitation of this procedure, which means we can approximate it as closely as we want.
[fn:2] Here we define a convex combination is a mapping from the production of two same regular space to the same space with parameter $\theta \in [0,1]$ :  $f(x_{1},x_{2}) = \theta x_{1} + (1-\theta)x_{2}$
